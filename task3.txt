import os
import sys
import json
import pandas as pd
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# æ·±åº¦å­¦ä¹ ç›¸å…³åº“
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
from torchvision import transforms, models
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau

# æœºå™¨å­¦ä¹ ç›¸å…³åº“
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from collections import Counter
import random

# ==================== 1. æ•°æ®åˆ†æ ====================
def analyze_dataset(data_dir):
    """
    å¯¹æ•°æ®é›†è¿›è¡Œå…¨é¢åˆ†æ
    """
    print("=" * 60)
    print("1. æ•°æ®é›†åˆ†æ")
    print("=" * 60)
    
    # æƒ…æ„Ÿç±»åˆ«æ˜ å°„
    emotion_mapping = {
        "0": "Angry",
        "1": "Fear", 
        "2": "Happy",
        "3": "Sad",
        "4": "Surprise",
        "5": "Neutral"
    }
    
    # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å›¾ç‰‡æ•°é‡
    class_counts = {}
    class_examples = {}
    total_images = 0
    
    train_dir = os.path.join(data_dir, "train")
    
    for emotion_id, emotion_name in emotion_mapping.items():
        emotion_dir = os.path.join(train_dir, emotion_name)
        if os.path.exists(emotion_dir):
            image_files = [f for f in os.listdir(emotion_dir) 
                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
            count = len(image_files)
            class_counts[emotion_name] = count
            total_images += count
            
            # ä¿å­˜ç¤ºä¾‹å›¾ç‰‡è·¯å¾„
            if image_files:
                class_examples[emotion_name] = os.path.join(emotion_dir, image_files[0])
    
    print(f"æ€»å›¾ç‰‡æ•°é‡: {total_images}")
    print("\nå„ç±»åˆ«åˆ†å¸ƒ:")
    for emotion, count in class_counts.items():
        percentage = (count / total_images) * 100
        print(f"  {emotion}: {count} å¼ å›¾ç‰‡ ({percentage:.1f}%)")
    
    # å¯è§†åŒ–ç±»åˆ«åˆ†å¸ƒ
    plt.figure(figsize=(10, 6))
    bars = plt.bar(class_counts.keys(), class_counts.values())
    plt.title('æ•°æ®é›†ç±»åˆ«åˆ†å¸ƒ', fontsize=16)
    plt.xlabel('æƒ…æ„Ÿç±»åˆ«', fontsize=12)
    plt.ylabel('å›¾ç‰‡æ•°é‡', fontsize=12)
    plt.xticks(rotation=45)
    
    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{int(height)}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig('dataset_distribution.png', dpi=100, bbox_inches='tight')
    plt.show()
    
    # æ£€æŸ¥æ•°æ®å‡è¡¡æ€§
    max_count = max(class_counts.values())
    min_count = min(class_counts.values())
    imbalance_ratio = max_count / min_count
    print(f"\næ•°æ®ä¸å‡è¡¡æ¯”ä¾‹: {imbalance_ratio:.2f}")
    
    if imbalance_ratio > 3:
        print("âš ï¸ è­¦å‘Š: æ•°æ®å­˜åœ¨æ˜æ˜¾ä¸å‡è¡¡ï¼Œå»ºè®®ä½¿ç”¨ç±»åˆ«æƒé‡æˆ–è¿‡é‡‡æ ·æŠ€æœ¯")
    
    return class_counts, emotion_mapping, class_examples

# ==================== 2. æ•°æ®é›†æ„å»ºï¼ˆå¸¦äº¤å‰éªŒè¯ï¼‰ ====================
class EmotionDataset(Dataset):
    """è‡ªå®šä¹‰æƒ…æ„Ÿæ•°æ®é›†ç±»"""
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        label = self.labels[idx]
        
        try:
            image = Image.open(img_path).convert('RGB')
            
            if self.transform:
                image = self.transform(image)
                
            return image, label
        except Exception as e:
            print(f"åŠ è½½å›¾ç‰‡å¤±è´¥: {img_path}, é”™è¯¯: {e}")
            # è¿”å›ç©ºç™½å›¾ç‰‡
            if self.transform:
                image = torch.zeros(3, 224, 224)
            else:
                image = Image.new('RGB', (224, 224))
            return image, label

def prepare_dataset(data_dir, emotion_mapping, use_kfold=True, n_splits=5):
    """
    å‡†å¤‡æ•°æ®é›†ï¼Œæ”¯æŒäº¤å‰éªŒè¯
    """
    print("\n" + "=" * 60)
    print("2. æ•°æ®é›†å‡†å¤‡")
    print("=" * 60)
    
    train_dir = os.path.join(data_dir, "train")
    
    # æ”¶é›†æ‰€æœ‰å›¾ç‰‡è·¯å¾„å’Œæ ‡ç­¾
    all_image_paths = []
    all_labels = []
    
    for emotion_id, emotion_name in emotion_mapping.items():
        emotion_dir = os.path.join(train_dir, emotion_name)
        if os.path.exists(emotion_dir):
            image_files = [f for f in os.listdir(emotion_dir) 
                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
            
            for img_file in image_files:
                img_path = os.path.join(emotion_dir, img_file)
                all_image_paths.append(img_path)
                all_labels.append(int(emotion_id))
    
    print(f"æ€»å…±æ”¶é›†åˆ° {len(all_image_paths)} å¼ å›¾ç‰‡")
    
    if use_kfold:
        print(f"ä½¿ç”¨ {n_splits} æŠ˜äº¤å‰éªŒè¯")
        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
        return all_image_paths, all_labels, skf
    else:
        # ä¸ä½¿ç”¨äº¤å‰éªŒè¯ï¼Œç›´æ¥åˆ’åˆ†
        train_paths, val_paths, train_labels, val_labels = train_test_split(
            all_image_paths, all_labels, 
            test_size=0.2, 
            stratify=all_labels,
            random_state=42
        )
        print(f"è®­ç»ƒé›†: {len(train_paths)} å¼ å›¾ç‰‡")
        print(f"éªŒè¯é›†: {len(val_paths)} å¼ å›¾ç‰‡")
        return (train_paths, val_paths, train_labels, val_labels)

# ==================== 3. ç‰¹å¾å·¥ç¨‹ ====================
class DataAugmentation:
    """æ•°æ®å¢å¼ºç­–ç•¥"""
    @staticmethod
    def get_train_transform():
        """è®­ç»ƒé›†å¢å¼º"""
        return transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.RandomCrop(224),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225]),
            transforms.RandomErasing(p=0.1, scale=(0.02, 0.1), value='random')
        ])
    
    @staticmethod
    def get_val_transform():
        """éªŒè¯é›†/æµ‹è¯•é›†è½¬æ¢"""
        return transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                               std=[0.229, 0.224, 0.225])
        ])

# ==================== 4. æ¨¡å‹æ¶æ„ ====================
class AttentionModule(nn.Module):
    """æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—"""
    def __init__(self, in_channels):
        super(AttentionModule, self).__init__()
        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        attention_weights = self.sigmoid(self.conv(x))
        return x * attention_weights

class EmotionCNN(nn.Module):
    """åŸºç¡€CNNæ¨¡å‹"""
    def __init__(self, num_classes=6):
        super(EmotionCNN, self).__init__()
        
        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(256)
        
        # æ³¨æ„åŠ›æ¨¡å—
        self.attention = AttentionModule(256)
        
        # æ± åŒ–å±‚
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(0.5)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(256 * 28 * 28, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, num_classes)
        
    def forward(self, x):
        # å·ç§¯å—1
        x = self.pool(F.relu(self.bn1(self.conv1(x))))
        x = self.dropout(x)
        
        # å·ç§¯å—2
        x = self.pool(F.relu(self.bn2(self.conv2(x))))
        x = self.dropout(x)
        
        # å·ç§¯å—3 + æ³¨æ„åŠ›
        x = self.pool(F.relu(self.bn3(self.conv3(x))))
        x = self.attention(x)
        x = self.dropout(x)
        
        # å±•å¹³
        x = x.view(x.size(0), -1)
        
        # å…¨è¿æ¥å±‚
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        
        return x

class EnhancedEmotionModel(nn.Module):
    """å¢å¼ºçš„æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼‰"""
    def __init__(self, num_classes=6, model_name='resnet50'):
        super(EnhancedEmotionModel, self).__init__()
        
        # é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹
        if model_name == 'resnet50':
            backbone = models.resnet50(pretrained=True)
            num_features = backbone.fc.in_features
            # ç§»é™¤æœ€åçš„å…¨è¿æ¥å±‚
            self.backbone = nn.Sequential(*list(backbone.children())[:-2])
            
        elif model_name == 'efficientnet':
            backbone = models.efficientnet_b0(pretrained=True)
            num_features = backbone.classifier[1].in_features
            self.backbone = nn.Sequential(*list(backbone.children())[:-2])
            
        elif model_name == 'mobilenet':
            backbone = models.mobilenet_v2(pretrained=True)
            num_features = backbone.classifier[1].in_features
            self.backbone = backbone.features
            
        # å…¨å±€å¹³å‡æ± åŒ–
        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.Sequential(
            nn.Linear(num_features, num_features // 2),
            nn.ReLU(),
            nn.Linear(num_features // 2, num_features),
            nn.Sigmoid()
        )
        
        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(num_features, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
    
    def forward(self, x):
        # ç‰¹å¾æå–
        features = self.backbone(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        pooled = self.global_avg_pool(features)
        pooled = pooled.view(pooled.size(0), -1)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = self.attention(pooled)
        attended = pooled * attention_weights
        
        # åˆ†ç±»
        output = self.classifier(attended)
        
        return output

class EnsembleModel(nn.Module):
    """é›†æˆæ¨¡å‹"""
    def __init__(self, num_classes=6):
        super(EnsembleModel, self).__init__()
        
        # ä¸‰ä¸ªä¸åŒçš„æ¨¡å‹
        self.model1 = EnhancedEmotionModel(num_classes, 'resnet50')
        self.model2 = EnhancedEmotionModel(num_classes, 'efficientnet')
        self.model3 = EnhancedEmotionModel(num_classes, 'mobilenet')
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(num_classes * 3, 128),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(128, num_classes)
        )
    
    def forward(self, x):
        out1 = self.model1(x)
        out2 = self.model2(x)
        out3 = self.model3(x)
        
        # æ‹¼æ¥ç‰¹å¾
        combined = torch.cat([out1, out2, out3], dim=1)
        
        # èåˆ
        output = self.fusion(combined)
        
        return output

# ==================== 5. è®­ç»ƒå’Œè¯„ä¼° ====================
class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    def __init__(self, model, device, class_weights=None):
        self.model = model
        self.device = device
        self.model.to(device)
        
        # æŸå¤±å‡½æ•°ï¼ˆå¸¦ç±»åˆ«æƒé‡å¤„ç†ä¸å‡è¡¡æ•°æ®ï¼‰
        if class_weights is not None:
            class_weights = torch.FloatTensor(class_weights).to(device)
            self.criterion = nn.CrossEntropyLoss(weight=class_weights)
        else:
            self.criterion = nn.CrossEntropyLoss()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = CosineAnnealingLR(self.optimizer, T_max=10, eta_min=1e-6)
        
        # è®°å½•è®­ç»ƒå†å²
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        pbar = tqdm(train_loader, desc="è®­ç»ƒ", leave=False)
        for images, labels in pbar:
            images, labels = images.to(self.device), labels.to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(images)
            loss = self.criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            
            # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            self.optimizer.step()
            
            # ç»Ÿè®¡
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': running_loss / (pbar.n + 1),
                'acc': 100. * correct / total
            })
        
        epoch_loss = running_loss / len(train_loader)
        epoch_acc = 100. * correct / total
        
        self.train_losses.append(epoch_loss)
        self.train_accs.append(epoch_acc)
        
        return epoch_loss, epoch_acc
    
    def validate(self, val_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc="éªŒè¯", leave=False)
            for images, labels in pbar:
                images, labels = images.to(self.device), labels.to(self.device)
                
                outputs = self.model(images)
                loss = self.criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                pbar.set_postfix({
                    'loss': running_loss / (pbar.n + 1),
                    'acc': 100. * correct / total
                })
        
        epoch_loss = running_loss / len(val_loader)
        epoch_acc = 100. * correct / total
        
        self.val_losses.append(epoch_loss)
        self.val_accs.append(epoch_acc)
        
        return epoch_loss, epoch_acc, all_preds, all_labels
    
    def train(self, train_loader, val_loader, num_epochs=10, patience=10):
        """å®Œæ•´è®­ç»ƒè¿‡ç¨‹"""
        print(f"\nå¼€å§‹è®­ç»ƒï¼Œå…± {num_epochs} ä¸ªepoch")
        print("-" * 60)
        
        best_val_acc = 0.0
        patience_counter = 0
        
        for epoch in range(num_epochs):
            print(f"\nEpoch {epoch+1}/{num_epochs}")
            
            # è®­ç»ƒ
            train_loss, train_acc = self.train_epoch(train_loader)
            
            # éªŒè¯
            val_loss, val_acc, preds, labels = self.validate(val_loader)
            
            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()
            
            # æ‰“å°ç»“æœ
            print(f"è®­ç»ƒ - æŸå¤±: {train_loss:.4f}, å‡†ç¡®ç‡: {train_acc:.2f}%")
            print(f"éªŒè¯ - æŸå¤±: {val_loss:.4f}, å‡†ç¡®ç‡: {val_acc:.2f}%")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                patience_counter = 0
                
                # ä¿å­˜æ¨¡å‹
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': self.model.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'val_accuracy': val_acc,
                    'train_accuracy': train_acc,
                }, 'best_model.pth')
                
                print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
            else:
                patience_counter += 1
                print(f"â³ éªŒè¯å‡†ç¡®ç‡æœªæå‡ ({patience_counter}/{patience})")
            
            # æ—©åœæ£€æŸ¥
            if patience_counter >= patience:
                print(f"ğŸ›‘ æ—©åœè§¦å‘")
                break
        
        print(f"\nè®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
        return best_val_acc

def evaluate_model(model, test_loader, device, emotion_mapping):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for images, labels in tqdm(test_loader, desc="æµ‹è¯•"):
            images = images.to(device)
            labels = labels.to(device)
            
            outputs = model(images)
            _, predicted = outputs.max(1)
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds, average='weighted')
    
    print(f"\næµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"æµ‹è¯•F1åˆ†æ•°: {f1:.4f}")
    
    # åˆ†ç±»æŠ¥å‘Š
    print("\nåˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_labels, all_preds, 
                              target_names=list(emotion_mapping.values())[:6]))
    
    # æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_preds)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=list(emotion_mapping.values())[:6],
                yticklabels=list(emotion_mapping.values())[:6])
    plt.title('æ··æ·†çŸ©é˜µ')
    plt.ylabel('çœŸå®æ ‡ç­¾')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=100, bbox_inches='tight')
    plt.show()
    
    return accuracy, f1

def visualize_training_history(trainer):
    """å¯è§†åŒ–è®­ç»ƒå†å²"""
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    # æŸå¤±æ›²çº¿
    axes[0].plot(trainer.train_losses, label='è®­ç»ƒæŸå¤±')
    axes[0].plot(trainer.val_losses, label='éªŒè¯æŸå¤±')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('æŸå¤±')
    axes[0].set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿')
    axes[0].legend()
    axes[0].grid(True)
    
    # å‡†ç¡®ç‡æ›²çº¿
    axes[1].plot(trainer.train_accs, label='è®­ç»ƒå‡†ç¡®ç‡')
    axes[1].plot(trainer.val_accs, label='éªŒè¯å‡†ç¡®ç‡')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('å‡†ç¡®ç‡ (%)')
    axes[1].set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡æ›²çº¿')
    axes[1].legend()
    axes[1].grid(True)
    
    plt.tight_layout()
    plt.savefig('training_history.png', dpi=100, bbox_inches='tight')
    plt.show()

# ==================== 6. ä¸»ç¨‹åº ====================
def main():
    # æ•°æ®é›†è·¯å¾„
    data_dir = "/kaggle/input/task3-dataset/fer_data"  # æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    
    # è®¾ç½®éšæœºç§å­
    seed = 42
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    
    # è®¾å¤‡é€‰æ‹©
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # 1. æ•°æ®åˆ†æ
    class_counts, emotion_mapping, class_examples = analyze_dataset(data_dir)
    
    # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆå¤„ç†ä¸å‡è¡¡æ•°æ®ï¼‰
    total = sum(class_counts.values())
    class_weights = [total / (len(class_counts) * count) 
                    for count in class_counts.values()]
    print(f"\nç±»åˆ«æƒé‡: {class_weights}")

    # 2. æ•°æ®é›†å‡†å¤‡ï¼ˆä½¿ç”¨äº¤å‰éªŒè¯ï¼‰
    use_kfold = True
    if use_kfold:
        all_paths, all_labels, skf = prepare_dataset(data_dir, emotion_mapping, use_kfold=True, n_splits=5)
        
        # äº¤å‰éªŒè¯è®­ç»ƒ
        fold_results = []
        for fold, (train_idx, val_idx) in enumerate(skf.split(all_paths, all_labels)):
            print(f"\n{'='*60}")
            print(f"ç¬¬ {fold+1} æŠ˜äº¤å‰éªŒè¯")
            print(f"{'='*60}")
            
            # åˆ’åˆ†æ•°æ®
            train_paths = [all_paths[i] for i in train_idx]
            val_paths = [all_paths[i] for i in val_idx]
            train_labels = [all_labels[i] for i in train_idx]
            val_labels = [all_labels[i] for i in val_idx]
            
            # åˆ›å»ºæ•°æ®é›†
            train_transform = DataAugmentation.get_train_transform()
            val_transform = DataAugmentation.get_val_transform()
            
            train_dataset = EmotionDataset(train_paths, train_labels, train_transform)
            val_dataset = EmotionDataset(val_paths, val_labels, val_transform)
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            batch_size = 32
            train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                                     shuffle=True, num_workers=4, pin_memory=True)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, 
                                   shuffle=False, num_workers=4, pin_memory=True)
            
            print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
            print(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
            
            # 3. åˆ›å»ºæ¨¡å‹
            model = EnhancedEmotionModel(num_classes=6, model_name='resnet50')
            
            # 4. è®­ç»ƒæ¨¡å‹
            trainer = ModelTrainer(model, device, class_weights)
            val_acc = trainer.train(train_loader, val_loader, num_epochs=30, patience=7)
            
            # è®°å½•ç»“æœ
            fold_results.append(val_acc)
            
            # å¯è§†åŒ–è®­ç»ƒå†å²
            visualize_training_history(trainer)
        
        # äº¤å‰éªŒè¯ç»“æœ
        print(f"\n{'='*60}")
        print("äº¤å‰éªŒè¯ç»“æœ")
        print(f"{'='*60}")
        for i, acc in enumerate(fold_results):
            print(f"ç¬¬ {i+1} æŠ˜éªŒè¯å‡†ç¡®ç‡: {acc:.2f}%")
        print(f"å¹³å‡éªŒè¯å‡†ç¡®ç‡: {np.mean(fold_results):.2f}% Â± {np.std(fold_results):.2f}%")
        
        # ä½¿ç”¨å®Œæ•´æ•°æ®é›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        print(f"\n{'='*60}")
        print("ä½¿ç”¨å®Œæ•´è®­ç»ƒé›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹")
        print(f"{'='*60}")
        
        # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
        train_transform = DataAugmentation.get_train_transform()
        train_dataset = EmotionDataset(all_paths, all_labels, train_transform)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, 
                                 num_workers=4, pin_memory=True)
        
        # åˆ›å»ºéªŒè¯é›†ï¼ˆç•™å‡º10%ï¼‰
        _, val_paths, _, val_labels = train_test_split(
            all_paths, all_labels, test_size=0.1, 
            stratify=all_labels, random_state=42
        )
        val_dataset = EmotionDataset(val_paths, val_labels, DataAugmentation.get_val_transform())
        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, 
                               num_workers=4, pin_memory=True)
        
        final_model = EnhancedEmotionModel(num_classes=6, model_name='resnet50')
        final_trainer = ModelTrainer(final_model, device, class_weights)
        final_acc = final_trainer.train(train_loader, val_loader, num_epochs=50, patience=10)
        
        print(f"\næœ€ç»ˆæ¨¡å‹éªŒè¯å‡†ç¡®ç‡: {final_acc:.2f}%")
        
    else:
        # ä¸ä½¿ç”¨äº¤å‰éªŒè¯
        train_paths, val_paths, train_labels, val_labels = prepare_dataset(
            data_dir, emotion_mapping, use_kfold=False
        )
        
        # åˆ›å»ºæ•°æ®é›†
        train_transform = DataAugmentation.get_train_transform()
        val_transform = DataAugmentation.get_val_transform()
        
        train_dataset = EmotionDataset(train_paths, train_labels, train_transform)
        val_dataset = EmotionDataset(val_paths, val_labels, val_transform)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        batch_size = 32
        train_loader = DataLoader(train_dataset, batch_size=batch_size, 
                                 shuffle=True, num_workers=4, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, 
                               shuffle=False, num_workers=4, pin_memory=True)
        
        # åˆ›å»ºæ¨¡å‹
        model = EnhancedEmotionModel(num_classes=6, model_name='resnet50')
        
        # è®­ç»ƒæ¨¡å‹
        trainer = ModelTrainer(model, device, class_weights)
        val_acc = trainer.train(train_loader, val_loader, num_epochs=50, patience=10)
        
        print(f"\næ¨¡å‹éªŒè¯å‡†ç¡®ç‡: {val_acc:.2f}%")
        
        # å¯è§†åŒ–è®­ç»ƒå†å²
        visualize_training_history(trainer)

    
    # åŠ è½½æµ‹è¯•é›†
    test_dir = os.path.join(data_dir, "test")
    print(f"æµ‹è¯•é›†è·¯å¾„: {test_dir}")

    if os.path.exists(test_dir):
        test_paths = []
        test_labels = []  # ç”¨-1å ä½
    
        # ç›´æ¥è¯»å–testç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
        for file in os.listdir(test_dir):
            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                img_path = os.path.join(test_dir, file)
                test_paths.append(img_path)
                test_labels.append(-1)  # å ä½ç¬¦
    
        if test_paths:
            print(f"æ‰¾åˆ° {len(test_paths)} å¼ æµ‹è¯•å›¾ç‰‡")
            test_dataset = EmotionDataset(test_paths, test_labels, DataAugmentation.get_val_transform())
            test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            #model = EnhancedEmotionModel(num_classes=6, model_name='resnet50')
            
            checkpoint = torch.load('best_model.pth', map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])

            # ç”Ÿæˆé¢„æµ‹ç»“æœ
            generate_predictions(model, test_dir, device, emotion_mapping)
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    generate_report(emotion_mapping, class_counts, 
                   np.mean(fold_results) if use_kfold else val_acc)

def generate_predictions(model, test_dir, device, emotion_mapping):
    """ç”Ÿæˆé¢„æµ‹ç»“æœæ–‡ä»¶"""
    print("\nç”Ÿæˆé¢„æµ‹ç»“æœ...")
    model.to(device)
    # æ”¶é›†æµ‹è¯•å›¾ç‰‡
    test_images = []
    for root, dirs, files in os.walk(test_dir):
        for file in files:
            if file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):
                test_images.append(os.path.join(root, file))
    
    if not test_images:
        print("æœªæ‰¾åˆ°æµ‹è¯•å›¾ç‰‡")
        return
    
    # é¢„æµ‹
    model.eval()
    predictions = []
    
    transform = DataAugmentation.get_val_transform()
    
    with torch.no_grad():
        for img_path in tqdm(test_images, desc="é¢„æµ‹"):
            try:
                image = Image.open(img_path).convert('RGB')
                image = transform(image).unsqueeze(0).to(device)
                
                output = model(image)
                _, predicted = output.max(1)
                
                predictions.append({
                    'ID': os.path.basename(img_path),
                    'Emotion': int(predicted.item())
                })
            except Exception as e:
                print(f"é¢„æµ‹å¤±è´¥ {img_path}: {e}")
    
    # ä¿å­˜ç»“æœ
    df = pd.DataFrame(predictions)
    df.to_csv('submission.csv', index=False)
    print(f"é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° submission.csv")
    
    # ç»Ÿè®¡é¢„æµ‹åˆ†å¸ƒ
    print("\né¢„æµ‹ç»“æœåˆ†å¸ƒ:")
    pred_counts = df['emotion'].value_counts()
    for emotion, count in pred_counts.items():
        percentage = count / len(df) * 100
        print(f"  {emotion}: {count} ({percentage:.1f}%)")

def generate_report(emotion_mapping, class_counts, val_accuracy):
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    report = f"""
    ====================================================
    å›¾åƒæƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡æœ€ç»ˆæŠ¥å‘Š
    ====================================================
    
    æ•°æ®é›†ä¿¡æ¯:
      æ€»ç±»åˆ«æ•°: 6
      ç±»åˆ«æ˜ å°„: {emotion_mapping}
    
    æ•°æ®é›†åˆ†å¸ƒ:
    """
    
    for emotion, count in class_counts.items():
        report += f"      {emotion}: {count} å¼ å›¾ç‰‡\n"
    
    report += f"""
    æ¨¡å‹æ€§èƒ½:
      æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.2f}%
    
    æ–‡ä»¶è¾“å‡º:
      1. æ•°æ®é›†åˆ†å¸ƒå›¾: dataset_distribution.png
      2. è®­ç»ƒå†å²å›¾: training_history.png
      3. æ··æ·†çŸ©é˜µ: confusion_matrix.png
      4. æœ€ä½³æ¨¡å‹: best_model.pth
      5. é¢„æµ‹ç»“æœ: submission.csv
    
    è®­ç»ƒå®Œæˆæ—¶é—´: {pd.Timestamp.now()}
    """
    
    with open('final_report.txt', 'w') as f:
        f.write(report)
    
    print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ° final_report.txt")

if __name__ == '__main__':
    main()